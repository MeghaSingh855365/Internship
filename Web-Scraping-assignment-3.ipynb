{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e90a1ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "import re\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fa4c43",
   "metadata": {},
   "source": [
    "# Q1: Write a python program which searches all the product under a particular product vertical from www.amazon.in. The product verticals to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a741ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Administrator\\Downloads\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2d7713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifiy the url of the webpage to be scrapped\n",
    "driver.get(\"https://www.amazon.in/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "557921f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product that you want to search : guitar\n"
     ]
    }
   ],
   "source": [
    "# entering the product that we want to search\n",
    "userInput = input('Enter the product that you want to search : ')\n",
    "#finding element for job search bar\n",
    "search = driver.find_element_by_id(\"twotabsearchtextbox\")\n",
    "# sending the user input to search bar\n",
    "search.send_keys(userInput)\n",
    "\n",
    "# locating the search button using xpath\n",
    "search_btn = driver.find_element_by_xpath(\"//div[@class='nav-search-submit nav-sprite']/span/input\")\n",
    "\n",
    "# clicking on search button\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e631d03",
   "metadata": {},
   "source": [
    "# Q2 : In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product vertical has less than 3 pages in search results then scrape all the products available under that product vertical. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Rating\", \"No. of Ratings\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\", \"Other Details\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28437e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching URLs to open the pages\n",
    "urls = []          # empty list\n",
    "for i in range(0,3):      # for loop to scrape 3 pages\n",
    "    page_url = driver.find_elements_by_xpath(\"//a[@class='a-link-normal a-text-normal']\")\n",
    "    for i in page_url:\n",
    "        urls.append(i.get_attribute(\"href\"))\n",
    "        next_btn = driver.find_element_by_xpath(\"//li[@class='a-last']/a\")\n",
    "        time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37885d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 219 219 219 219 219 219 219\n"
     ]
    }
   ],
   "source": [
    "# making empty list and fetching required data\n",
    "brand_name = []\n",
    "product_name = []\n",
    "ratings = []\n",
    "num_ratings = []\n",
    "prices = []\n",
    "exchange = []\n",
    "exp_delivery = []\n",
    "availability = []\n",
    "otherDetails = []\n",
    "\n",
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    #fetching brand name \n",
    "    try:\n",
    "        brand = driver.find_element_by_xpath(\"//a[@id='bylineInfo']\")\n",
    "        brand_name.append(brand.text)\n",
    "    except NoSuchElementException:\n",
    "        brand_name.append('-')\n",
    "    \n",
    "    \n",
    "    # fetching Name of the Product\n",
    "    try:\n",
    "        product = driver.find_element_by_xpath(\"//span[@id='productTitle']\")\n",
    "        product_name.append(product.text)\n",
    "    except NoSuchElementException:\n",
    "        product_name.append('-')\n",
    "        \n",
    "        \n",
    "\n",
    "     #fetching ratings\n",
    "    try:\n",
    "        rating = driver.find_element_by_xpath(\"//span[@class='a-size-base a-nowrap']/span\")\n",
    "        ratings.append(rating.text)\n",
    "    except NoSuchElementException:\n",
    "        ratings.append('-')\n",
    "        \n",
    " \n",
    "    #fetching  no of ratings\n",
    "    try:\n",
    "        num_rating = driver.find_element_by_xpath(\"//span[@id='acrCustomerReviewText']\")\n",
    "        num_ratings.append(num_rating.text)\n",
    "    except NoSuchElementException:\n",
    "        num_ratings.append('-')\n",
    "        \n",
    "\n",
    "    #fetching price of the product\n",
    "    try:\n",
    "        price = driver.find_element_by_xpath(\"//td[@class='a-span12']\")\n",
    "        prices.append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        prices.append('-')\n",
    "        \n",
    "        \n",
    "    #fetching return/exchange\n",
    "    try:\n",
    "        exch = driver.find_element_by_xpath(\"//span[@class='a-declarative']/div/a\")\n",
    "        exchange.append(exch.text)\n",
    "    except NoSuchElementException:\n",
    "        exchange.append('-')\n",
    "        \n",
    "\n",
    "    #fetching expected delivery\n",
    "    try:\n",
    "        delivery = driver.find_element_by_xpath(\"//div[@class='a-section a-spacing-mini']/b\")\n",
    "        exp_delivery.append(delivery.text)\n",
    "    except NoSuchElementException:\n",
    "        exp_delivery.append('-')\n",
    "        \n",
    "\n",
    "    #fetching availability information\n",
    "    try:\n",
    "        avail = driver.find_element_by_xpath(\"//span[@class='a-size-medium a-color-success']\")\n",
    "        availability.append(avail.text)\n",
    "    except NoSuchElementException:\n",
    "        availability.append('-')\n",
    "        \n",
    "    #other details\n",
    "    try:\n",
    "        oth_det = driver.find_element_by_xpath(\"//ul[@class='a-unordered-list a-vertical a-spacing-mini']\")\n",
    "        otherDetails.append(oth_det.text)\n",
    "    except NoSuchElementException:\n",
    "        otherDetails.append('-')\n",
    "        \n",
    "print(len(brand_name),len(product_name),len(ratings),len(prices),len(exchange),len(exp_delivery),len(availability),\n",
    "len(otherDetails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7c94164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Name of the Product</th>\n",
       "      <th>Rating</th>\n",
       "      <th>No. of Ratings</th>\n",
       "      <th>Price</th>\n",
       "      <th>Return/Exchange</th>\n",
       "      <th>Expected Delivery</th>\n",
       "      <th>Availability</th>\n",
       "      <th>Other Details</th>\n",
       "      <th>Product URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Visit the Kadence Store</td>\n",
       "      <td>Kadence Slowhand Premium Jumbo Semi Acoustic G...</td>\n",
       "      <td>4.1 out of 5</td>\n",
       "      <td>302 ratings</td>\n",
       "      <td>₹9,999.00</td>\n",
       "      <td>7 Days Replacement</td>\n",
       "      <td>Thursday, Dec 16</td>\n",
       "      <td>In stock.</td>\n",
       "      <td>ACOUSTIC-ELECTRIC GUITAR: Kadence Slowhand sem...</td>\n",
       "      <td>https://www.amazon.in/gp/slredirect/picassoRed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Visit the Kadence Store</td>\n",
       "      <td>Kadence Frontier Series, Acoustic Guitar With/...</td>\n",
       "      <td>4 out of 5</td>\n",
       "      <td>1,233 ratings</td>\n",
       "      <td>₹4,999.00</td>\n",
       "      <td>7 Days Replacement</td>\n",
       "      <td>Thursday, Dec 16</td>\n",
       "      <td>In stock.</td>\n",
       "      <td>★ Design - The High Gloss finish, Simple head ...</td>\n",
       "      <td>https://www.amazon.in/gp/slredirect/picassoRed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brand: Medellin</td>\n",
       "      <td>Medellin MED-BLU-C Linden Wood Acoustic Guitar</td>\n",
       "      <td>4 out of 5</td>\n",
       "      <td>550 ratings</td>\n",
       "      <td>₹2,399.00</td>\n",
       "      <td>7 Days Replacement</td>\n",
       "      <td>Wednesday, Dec 15</td>\n",
       "      <td>In stock.</td>\n",
       "      <td>Material: Wood\\nColour: Blue\\nAcoustic Guitar</td>\n",
       "      <td>https://www.amazon.in/gp/slredirect/picassoRed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Visit the Giuson Store</td>\n",
       "      <td>Giuson Venus Black 41 Inch Rosewood Fretboard ...</td>\n",
       "      <td>3.8 out of 5</td>\n",
       "      <td>369 ratings</td>\n",
       "      <td>₹3,599.00</td>\n",
       "      <td>7 Days Replacement</td>\n",
       "      <td>Dec 25 - 28</td>\n",
       "      <td>-</td>\n",
       "      <td>Product Sold by K-Retail</td>\n",
       "      <td>https://www.amazon.in/gp/slredirect/picassoRed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Visit the Intern Store</td>\n",
       "      <td>Intern INT-38C Acoustic Guitar Kit, With Bag, ...</td>\n",
       "      <td>4 out of 5</td>\n",
       "      <td>8,412 ratings</td>\n",
       "      <td>₹2,350.00</td>\n",
       "      <td>7 Days Replacement</td>\n",
       "      <td>Thursday, Dec 16</td>\n",
       "      <td>In stock.</td>\n",
       "      <td>Great looks with an innovative design to produ...</td>\n",
       "      <td>https://www.amazon.in/Intern-INT-38C-Acoustic-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>Visit the Kadence Store</td>\n",
       "      <td>Kadence Slowhand Series Premium Acoustic Guita...</td>\n",
       "      <td>3.1 out of 5</td>\n",
       "      <td>6 ratings</td>\n",
       "      <td>₹8,923.00</td>\n",
       "      <td>7 Days Replacement</td>\n",
       "      <td>Thursday, Dec 16</td>\n",
       "      <td>In stock.</td>\n",
       "      <td>★ Design - The Deep Pore paint finish, Simple ...</td>\n",
       "      <td>https://www.amazon.in/Kadence-Slowhand-Premium...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>Visit the VAULT Store</td>\n",
       "      <td>Vault ST1 Premium Electric Guitar - White</td>\n",
       "      <td>3.9 out of 5</td>\n",
       "      <td>28 ratings</td>\n",
       "      <td>₹7,219.00</td>\n",
       "      <td>7 Days Replacement</td>\n",
       "      <td>Thursday, Dec 16</td>\n",
       "      <td>In stock.</td>\n",
       "      <td>Sycamore Body, Maple neck, Rosewood fretboard,...</td>\n",
       "      <td>https://www.amazon.in/gp/slredirect/picassoRed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Visit the VAULT Store</td>\n",
       "      <td>Vault EA20CE 40 Inch Cutaway Electro Acoustic ...</td>\n",
       "      <td>3.6 out of 5</td>\n",
       "      <td>4 ratings</td>\n",
       "      <td>₹5,319.00</td>\n",
       "      <td>7 Days Replacement</td>\n",
       "      <td>Thursday, Dec 16</td>\n",
       "      <td>In stock.</td>\n",
       "      <td>Cutaway Design for reaching higher notes with ...</td>\n",
       "      <td>https://www.amazon.in/gp/slredirect/picassoRed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>Visit the Giuson Store</td>\n",
       "      <td>Giuson Venus Pinewood Acoustic Guitar With Bag...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>₹3,990.00</td>\n",
       "      <td>7 Days Replacement</td>\n",
       "      <td>Dec 19 - 22</td>\n",
       "      <td>-</td>\n",
       "      <td>41 inch Cut-A-Way Acoustic Guitar, Fret board:...</td>\n",
       "      <td>https://www.amazon.in/gp/slredirect/picassoRed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>Visit the Giuson Store</td>\n",
       "      <td>Giuson Electra Rosewood Fretboard Acoustic Gui...</td>\n",
       "      <td>4.7 out of 5</td>\n",
       "      <td>4 ratings</td>\n",
       "      <td>₹3,599.00</td>\n",
       "      <td>7 Days Replacement</td>\n",
       "      <td>Dec 22 - 25</td>\n",
       "      <td>-</td>\n",
       "      <td>Inbuilt pick up to plug and play with amplifie...</td>\n",
       "      <td>https://www.amazon.in/gp/slredirect/picassoRed...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>219 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Brand Name  \\\n",
       "0    Visit the Kadence Store   \n",
       "1    Visit the Kadence Store   \n",
       "2            Brand: Medellin   \n",
       "3     Visit the Giuson Store   \n",
       "4     Visit the Intern Store   \n",
       "..                       ...   \n",
       "214  Visit the Kadence Store   \n",
       "215    Visit the VAULT Store   \n",
       "216    Visit the VAULT Store   \n",
       "217   Visit the Giuson Store   \n",
       "218   Visit the Giuson Store   \n",
       "\n",
       "                                   Name of the Product        Rating  \\\n",
       "0    Kadence Slowhand Premium Jumbo Semi Acoustic G...  4.1 out of 5   \n",
       "1    Kadence Frontier Series, Acoustic Guitar With/...    4 out of 5   \n",
       "2       Medellin MED-BLU-C Linden Wood Acoustic Guitar    4 out of 5   \n",
       "3    Giuson Venus Black 41 Inch Rosewood Fretboard ...  3.8 out of 5   \n",
       "4    Intern INT-38C Acoustic Guitar Kit, With Bag, ...    4 out of 5   \n",
       "..                                                 ...           ...   \n",
       "214  Kadence Slowhand Series Premium Acoustic Guita...  3.1 out of 5   \n",
       "215          Vault ST1 Premium Electric Guitar - White  3.9 out of 5   \n",
       "216  Vault EA20CE 40 Inch Cutaway Electro Acoustic ...  3.6 out of 5   \n",
       "217  Giuson Venus Pinewood Acoustic Guitar With Bag...             -   \n",
       "218  Giuson Electra Rosewood Fretboard Acoustic Gui...  4.7 out of 5   \n",
       "\n",
       "    No. of Ratings      Price     Return/Exchange  Expected Delivery  \\\n",
       "0      302 ratings  ₹9,999.00  7 Days Replacement   Thursday, Dec 16   \n",
       "1    1,233 ratings  ₹4,999.00  7 Days Replacement   Thursday, Dec 16   \n",
       "2      550 ratings  ₹2,399.00  7 Days Replacement  Wednesday, Dec 15   \n",
       "3      369 ratings  ₹3,599.00  7 Days Replacement        Dec 25 - 28   \n",
       "4    8,412 ratings  ₹2,350.00  7 Days Replacement   Thursday, Dec 16   \n",
       "..             ...        ...                 ...                ...   \n",
       "214      6 ratings  ₹8,923.00  7 Days Replacement   Thursday, Dec 16   \n",
       "215     28 ratings  ₹7,219.00  7 Days Replacement   Thursday, Dec 16   \n",
       "216      4 ratings  ₹5,319.00  7 Days Replacement   Thursday, Dec 16   \n",
       "217              -  ₹3,990.00  7 Days Replacement        Dec 19 - 22   \n",
       "218      4 ratings  ₹3,599.00  7 Days Replacement        Dec 22 - 25   \n",
       "\n",
       "    Availability                                      Other Details  \\\n",
       "0      In stock.  ACOUSTIC-ELECTRIC GUITAR: Kadence Slowhand sem...   \n",
       "1      In stock.  ★ Design - The High Gloss finish, Simple head ...   \n",
       "2      In stock.      Material: Wood\\nColour: Blue\\nAcoustic Guitar   \n",
       "3              -                           Product Sold by K-Retail   \n",
       "4      In stock.  Great looks with an innovative design to produ...   \n",
       "..           ...                                                ...   \n",
       "214    In stock.  ★ Design - The Deep Pore paint finish, Simple ...   \n",
       "215    In stock.  Sycamore Body, Maple neck, Rosewood fretboard,...   \n",
       "216    In stock.  Cutaway Design for reaching higher notes with ...   \n",
       "217            -  41 inch Cut-A-Way Acoustic Guitar, Fret board:...   \n",
       "218            -  Inbuilt pick up to plug and play with amplifie...   \n",
       "\n",
       "                                           Product URL  \n",
       "0    https://www.amazon.in/gp/slredirect/picassoRed...  \n",
       "1    https://www.amazon.in/gp/slredirect/picassoRed...  \n",
       "2    https://www.amazon.in/gp/slredirect/picassoRed...  \n",
       "3    https://www.amazon.in/gp/slredirect/picassoRed...  \n",
       "4    https://www.amazon.in/Intern-INT-38C-Acoustic-...  \n",
       "..                                                 ...  \n",
       "214  https://www.amazon.in/Kadence-Slowhand-Premium...  \n",
       "215  https://www.amazon.in/gp/slredirect/picassoRed...  \n",
       "216  https://www.amazon.in/gp/slredirect/picassoRed...  \n",
       "217  https://www.amazon.in/gp/slredirect/picassoRed...  \n",
       "218  https://www.amazon.in/gp/slredirect/picassoRed...  \n",
       "\n",
       "[219 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the DataFrame for the scraped data\n",
    "\n",
    "guitar = pd.DataFrame({})\n",
    "guitar['Brand Name'] = brand_name\n",
    "guitar['Name of the Product'] = product_name\n",
    "guitar['Rating'] = ratings\n",
    "guitar['No. of Ratings'] = num_ratings\n",
    "guitar['Price'] = prices\n",
    "guitar['Return/Exchange'] = exchange\n",
    "guitar['Expected Delivery'] = exp_delivery\n",
    "guitar['Availability'] = availability\n",
    "guitar['Other Details'] = otherDetails\n",
    "guitar['Product URL'] = urls\n",
    "guitar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6454d72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the data in csv\n",
    "guitar.to_csv(\"Guitar.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb858ba",
   "metadata": {},
   "source": [
    "# Q3 : Write a python program to access the search bar and search button on images.google.com and scrape 100 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3825a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Administrator\\Downloads\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30ad6443",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://images.google.com/\")\n",
    "search_bar = driver.find_element_by_xpath('//*[@id=\"sbtc\"]/div/div[2]/input')    # Finding the search bar using it's xpath\n",
    "search_bar.send_keys(\"fruits\")       # Inputing \"banana\" keyword to search rock images\n",
    "search_button = driver.find_element_by_xpath('//*[@id=\"sbtc\"]/button')    # Finding the xpath of search button\n",
    "search_button.click()        # Clicking the search button\n",
    "# 500 time we scroll down by 10000 in order to generate more images on the website\n",
    "for _ in range(500):\n",
    "    driver.execute_script(\"window.scrollBy(0,10000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "059842b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "img_urls = []\n",
    "img_data = []\n",
    "for image in images:\n",
    "    source= image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "len(img_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f430b46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 0 of 100 images\n",
      "Downloading 1 of 100 images\n",
      "Downloading 2 of 100 images\n",
      "Downloading 3 of 100 images\n",
      "Downloading 4 of 100 images\n",
      "Downloading 5 of 100 images\n",
      "Downloading 6 of 100 images\n",
      "Downloading 7 of 100 images\n",
      "Downloading 8 of 100 images\n",
      "Downloading 9 of 100 images\n",
      "Downloading 10 of 100 images\n",
      "Downloading 11 of 100 images\n",
      "Downloading 12 of 100 images\n",
      "Downloading 13 of 100 images\n",
      "Downloading 14 of 100 images\n",
      "Downloading 15 of 100 images\n",
      "Downloading 16 of 100 images\n",
      "Downloading 17 of 100 images\n",
      "Downloading 18 of 100 images\n",
      "Downloading 19 of 100 images\n",
      "Downloading 20 of 100 images\n",
      "Downloading 21 of 100 images\n",
      "Downloading 22 of 100 images\n",
      "Downloading 23 of 100 images\n",
      "Downloading 24 of 100 images\n",
      "Downloading 25 of 100 images\n",
      "Downloading 26 of 100 images\n",
      "Downloading 27 of 100 images\n",
      "Downloading 28 of 100 images\n",
      "Downloading 29 of 100 images\n",
      "Downloading 30 of 100 images\n",
      "Downloading 31 of 100 images\n",
      "Downloading 32 of 100 images\n",
      "Downloading 33 of 100 images\n",
      "Downloading 34 of 100 images\n",
      "Downloading 35 of 100 images\n",
      "Downloading 36 of 100 images\n",
      "Downloading 37 of 100 images\n",
      "Downloading 38 of 100 images\n",
      "Downloading 39 of 100 images\n",
      "Downloading 40 of 100 images\n",
      "Downloading 41 of 100 images\n",
      "Downloading 42 of 100 images\n",
      "Downloading 43 of 100 images\n",
      "Downloading 44 of 100 images\n",
      "Downloading 45 of 100 images\n",
      "Downloading 46 of 100 images\n",
      "Downloading 47 of 100 images\n",
      "Downloading 48 of 100 images\n",
      "Downloading 49 of 100 images\n",
      "Downloading 50 of 100 images\n",
      "Downloading 51 of 100 images\n",
      "Downloading 52 of 100 images\n",
      "Downloading 53 of 100 images\n",
      "Downloading 54 of 100 images\n",
      "Downloading 55 of 100 images\n",
      "Downloading 56 of 100 images\n",
      "Downloading 57 of 100 images\n",
      "Downloading 58 of 100 images\n",
      "Downloading 59 of 100 images\n",
      "Downloading 60 of 100 images\n",
      "Downloading 61 of 100 images\n",
      "Downloading 62 of 100 images\n",
      "Downloading 63 of 100 images\n",
      "Downloading 64 of 100 images\n",
      "Downloading 65 of 100 images\n",
      "Downloading 66 of 100 images\n",
      "Downloading 67 of 100 images\n",
      "Downloading 68 of 100 images\n",
      "Downloading 69 of 100 images\n",
      "Downloading 70 of 100 images\n",
      "Downloading 71 of 100 images\n",
      "Downloading 72 of 100 images\n",
      "Downloading 73 of 100 images\n",
      "Downloading 74 of 100 images\n",
      "Downloading 75 of 100 images\n",
      "Downloading 76 of 100 images\n",
      "Downloading 77 of 100 images\n",
      "Downloading 78 of 100 images\n",
      "Downloading 79 of 100 images\n",
      "Downloading 80 of 100 images\n",
      "Downloading 81 of 100 images\n",
      "Downloading 82 of 100 images\n",
      "Downloading 83 of 100 images\n",
      "Downloading 84 of 100 images\n",
      "Downloading 85 of 100 images\n",
      "Downloading 86 of 100 images\n",
      "Downloading 87 of 100 images\n",
      "Downloading 88 of 100 images\n",
      "Downloading 89 of 100 images\n",
      "Downloading 90 of 100 images\n",
      "Downloading 91 of 100 images\n",
      "Downloading 92 of 100 images\n",
      "Downloading 93 of 100 images\n",
      "Downloading 94 of 100 images\n",
      "Downloading 95 of 100 images\n",
      "Downloading 96 of 100 images\n",
      "Downloading 97 of 100 images\n",
      "Downloading 98 of 100 images\n",
      "Downloading 99 of 100 images\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(img_urls)):\n",
    "    if i >= 100:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\" .format(i, 100))\n",
    "    response= requests.get(img_urls[i])\n",
    "    file = open(\"G:/fliprobo/img\"+str(i)+\".jpg\", \"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d400ee96",
   "metadata": {},
   "source": [
    "# Q4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Display Resolution”, “Processor”, “Processor Cores”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "908cbf62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetching urls of phones coming on 1st page\n",
    "flipkart_urls = []\n",
    "urls = driver.find_elements_by_xpath('//a[@class=\"_1fQZEK\"]')\n",
    "for i in urls:\n",
    "    flipkart_urls.append(i.get_attribute(\"href\"))\n",
    "len(flipkart_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ff41ffdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "flip_dict = {}\n",
    "flip_dict[\"Brand\"] = []\n",
    "flip_dict[\"Smartphone\"] = []\n",
    "flip_dict[\"Colour\"] = []\n",
    "flip_dict[\"RAM\"] = []\n",
    "flip_dict[\"Storage(ROM)\"] = []\n",
    "flip_dict[\"Primary Camera\"] = []\n",
    "flip_dict[\"Secondary Camera\"] = []\n",
    "flip_dict[\"Display Size\"] = []\n",
    "flip_dict[\"Display Resolution\"] = []\n",
    "flip_dict[\"Processor\"] = []\n",
    "flip_dict[\"Processor Cores\"] = []\n",
    "flip_dict[\"Battery Capacity\"] = []\n",
    "flip_dict[\"Battery Type\"] = []\n",
    "flip_dict[\"Price\"] = []\n",
    "flip_dict[\"URL\"] = []\n",
    "\n",
    "# Scraping data from each url\n",
    "for url in flipkart_urls:\n",
    "    driver.get(url)    # Saving url                                                     \n",
    "    print(\"Scraping URL = \", url)\n",
    "    flip_dict['URL'].append(url)                                                          # Loading the webpage by url\n",
    "    time.sleep(2)\n",
    "    \n",
    "    try:\n",
    "        read_more = driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _1FH0tX\"]')     # Button for expanding the specs\n",
    "        read_more.click()\n",
    "    except NoSuchElementException:\n",
    "        print(\"Exception Occured. Moving to next page\")\n",
    "    \n",
    "    try:\n",
    "        brand = driver.find_element_by_xpath('//span[@class=\"B_NuCI\"]')      # Extracting Brand from xpath\n",
    "        flip_dict[\"Brand\"].append(brand.text.split()[0])\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Brand'].append('-')\n",
    "        \n",
    "    try:\n",
    "        price = driver.find_element_by_xpath('//div[@class=\"_30jeq3 _16Jk6d\"]')      # Extracting Brand from xpath\n",
    "        flip_dict['Price'].append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Price'].append('-')\n",
    "        \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][1]/table/tbody/tr[3]/td[2]/ul/li')      # Extracting Brand from xpath\n",
    "        flip_dict['Smartphone'].append(name.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Smartphone'].append('-')\n",
    "    \n",
    "    try:\n",
    "        color = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][1]/table/tbody/tr[4]/td[2]/ul/li')      # Extracting Name from xpath\n",
    "        flip_dict['Colour'].append(color.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Colour'].append('-')\n",
    "    \n",
    "    try:\n",
    "        disp_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/div')\n",
    "        if disp_chk.text != \"Display Features\" : raise NoSuchElementException\n",
    "        disp_size = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/table[1]/tbody/tr[1]/td[2]/ul/li')  # Extracting Ratings from xpath\n",
    "        flip_dict['Display Size'].append(disp_size.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Display Size'].append('-')\n",
    "    \n",
    "    try:\n",
    "        disp_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/div')\n",
    "        if disp_chk.text != \"Display Features\" : raise NoSuchElementException\n",
    "        disp_res = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/table[1]/tbody/tr[2]/td[2]/ul/li')     # Extracting no. of Ratings from xpath\n",
    "        flip_dict['Display Resolution'].append(disp_res.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Display Resolution'].append('-')\n",
    "    \n",
    "    try:\n",
    "        pro_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[1]')\n",
    "        if pro_chk.text != \"Processor Type\" : raise NoSuchElementException\n",
    "        processor = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[2]/ul/li')   # Extracting Price from xpath\n",
    "        flip_dict['Processor'].append(processor.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Processor'].append('-')\n",
    "    \n",
    "    try:                                                                                     # Extracting Return/Exchange policy from xpath\n",
    "        core_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[3]/td[1]')\n",
    "        if core_chk.text != \"Processor Core\" :\n",
    "            core_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[1]')\n",
    "            if core_chk.text != \"Processor Core\" : \n",
    "                raise NoSuchElementException\n",
    "            else :\n",
    "                cores = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[2]/ul/li')\n",
    "        else :\n",
    "            cores = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[3]/td[2]/ul/li')\n",
    "        flip_dict['Processor Cores'].append(cores.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Processor Cores'].append('-')\n",
    "    \n",
    "    try:\n",
    "        rom = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][4]/table[1]/tbody/tr[1]/td[2]/ul/li')         # Extracting Expected Delivery from xpath\n",
    "        flip_dict['Storage(ROM)'].append(rom.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Storage(ROM)'].append('-')\n",
    "    \n",
    "    try:\n",
    "        ram = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][4]/table[1]/tbody/tr[2]/td[2]/ul/li')                # Extracting Availability from xpath\n",
    "        flip_dict['RAM'].append(ram.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['RAM'].append('-')\n",
    "    \n",
    "    try:                                                                                    # Extracting Other Details from xpath\n",
    "        pri_cam = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[2]/td[2]/ul/li')\n",
    "        flip_dict['Primary Camera'].append(pri_cam.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Primary Camera'].append('-')\n",
    "    \n",
    "    try:                                                                                    # Extracting Other Details from xpath\n",
    "        cam_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[6]/td[1]')\n",
    "        if cam_chk != \"Secondary Camera\" : \n",
    "            if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[5]/td[1]').text == \"Secondary Camera\":\n",
    "                sec_cam = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[5]/td[2]/ul/li')\n",
    "            else :\n",
    "                raise NoSuchElementException\n",
    "        else :\n",
    "            sec_cam = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[6]/td[2]/ul/li')\n",
    "        flip_dict['Secondary Camera'].append(sec_cam.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Secondary Camera'].append('-')\n",
    "        \n",
    "    try:\n",
    "        if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/div').text != \"Battery & Power Features\" :\n",
    "            if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/div').text == \"Battery & Power Features\" :\n",
    "                bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr/td[1]')\n",
    "                if bat_chk.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "                bat_cap = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr/td[2]/ul/li')                # Extracting Availability from xpath\n",
    "            elif driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/div').text == \"Battery & Power Features\" :\n",
    "                bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr/td[1]')\n",
    "                if bat_chk.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "                bat_cap = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr/td[2]/ul/li')\n",
    "            else:\n",
    "                raise NoSuchElementException\n",
    "        else :\n",
    "            bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr/td[1]')\n",
    "            if bat_chk.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "            bat_cap = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr/td[2]/ul/li')                # Extracting Availability from xpath\n",
    "        flip_dict['Battery Capacity'].append(bat_cap.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Battery Capacity'].append('-')\n",
    "    \n",
    "    try:\n",
    "        if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/div').text != \"Battery & Power Features\" :\n",
    "            if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/div').text == \"Battery & Power Features\" :\n",
    "                bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr[2]/td[1]')\n",
    "                if bat_chk.text != \"Battery Type\" : raise NoSuchElementException\n",
    "                bat_typ = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "            elif driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/div').text == \"Battery & Power Features\" :\n",
    "                bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr[2]/td[1]')\n",
    "                if bat_chk.text != \"Battery Type\" : raise NoSuchElementException\n",
    "                bat_typ = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "            else:\n",
    "                raise NoSuchElementException\n",
    "        else :\n",
    "            bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr[2]/td[1]')\n",
    "            if bat_chk.text != \"Battery Type\" : raise NoSuchElementException\n",
    "            bat_typ = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr[2]/td[2]/ul/li')                # Extracting Availability from xpath\n",
    "        flip_dict['Battery Type'].append(bat_typ.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Battery Type'].append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeadef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(flip_dict[\"Brand\"]), len(flip_dict[\"Smartphone\"]), len(flip_dict[\"Processor\"]), len(flip_dict[\"Price\"]), len(flip_dict['URL']))\n",
    "flipkart_df = pd.DataFrame.from_dict(flip_dict)\n",
    "flipkart_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143f78cd",
   "metadata": {},
   "source": [
    "# Q5.Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f934e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Administrator\\Downloads\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fd8943e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the City Name you want to search : delhi\n",
      "URL Extracted:  https://www.google.co.in/maps/place/Delhi/@28.6472799,76.8130574,10z/data=!3m1!4b1!4m5!3m4!1s0x390cfd5b347eb62d:0x37205b715389640!8m2!3d28.7040592!4d77.1024902\n",
      "Latitude = 28.6472799, Longitude = 76.8130574\n"
     ]
    }
   ],
   "source": [
    "# opening google maps\n",
    "driver.get(\"https://www.google.co.in/maps\")\n",
    "time.sleep(3)\n",
    "\n",
    "city = input('Enter the City Name you want to search : ')                                         # Enter city to be searched\n",
    "search = driver.find_element_by_id(\"searchboxinput\")                       # locating search bar\n",
    "search.clear()                                                             # clearing search bar\n",
    "time.sleep(2)\n",
    "search.send_keys(city)                                                     # entering values in search bar\n",
    "button = driver.find_element_by_id(\"searchbox-searchbutton\")               # locating search button\n",
    "button.click()                                                             # clicking search button\n",
    "time.sleep(3)\n",
    "\n",
    "try:\n",
    "    url_string = driver.current_url\n",
    "    print(\"URL Extracted: \", url_string)\n",
    "    lat_lng = re.findall(r'@(.*)data',url_string)\n",
    "    if len(lat_lng):\n",
    "        lat_long_list = lat_lng[0].split(\",\")\n",
    "        if len(lat_long_list)>=2:\n",
    "            lat = lat_long_list[0]\n",
    "            lng = lat_long_list[1]\n",
    "        print(\"Latitude = {}, Longitude = {}\".format(lat, lng))\n",
    "\n",
    "except Exception as e:\n",
    "        print(\"Error: \", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a50928",
   "metadata": {},
   "source": [
    "# Q6. Write a program to scrap details of all the funding deals for second quarter (i.e. July 20 – September 20) from trak.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "efb9598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Administrator\\Downloads\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ff286f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://trak.in/')\n",
    "button = driver.find_element_by_xpath('//li[@id=\"menu-item-51510\"]/a').get_attribute('href')\n",
    "driver.get(button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b143484e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Startup Name</th>\n",
       "      <th>Industry/Vertical</th>\n",
       "      <th>Sub-Vertical</th>\n",
       "      <th>Location</th>\n",
       "      <th>Investor</th>\n",
       "      <th>Investment Type</th>\n",
       "      <th>Amount(in USD)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15/07/2020</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Walmart Inc</td>\n",
       "      <td>M&amp;A</td>\n",
       "      <td>1,200,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16/07/2020</td>\n",
       "      <td>Vedantu</td>\n",
       "      <td>EduTech</td>\n",
       "      <td>Online Tutoring</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Coatue Management</td>\n",
       "      <td>Series D</td>\n",
       "      <td>100,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16/07/2020</td>\n",
       "      <td>Crio</td>\n",
       "      <td>EduTech</td>\n",
       "      <td>Learning Platform for Developers</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>021 Capital</td>\n",
       "      <td>pre-Series A</td>\n",
       "      <td>934,160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14/07/2020</td>\n",
       "      <td>goDutch</td>\n",
       "      <td>FinTech</td>\n",
       "      <td>Group Payments</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Matrix India,Y Combinator, Global Founders Cap...</td>\n",
       "      <td>Seed</td>\n",
       "      <td>1,700,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13/07/2020</td>\n",
       "      <td>Mystifly</td>\n",
       "      <td>Airfare Marketplace</td>\n",
       "      <td>Ticketing, Airline Retailing, and Post-Ticketi...</td>\n",
       "      <td>Singapore and Bangalore</td>\n",
       "      <td>Recruit Co. Ltd.</td>\n",
       "      <td>pre-Series B</td>\n",
       "      <td>3,300,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>JetSynthesys</td>\n",
       "      <td>Gaming and Entertainment</td>\n",
       "      <td>Gaming and Entertainment</td>\n",
       "      <td>Pune</td>\n",
       "      <td>Adar Poonawalla and Kris Gopalakrishnan.</td>\n",
       "      <td>Venture-Series Unknown</td>\n",
       "      <td>400,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10/07/2020</td>\n",
       "      <td>gigIndia</td>\n",
       "      <td>Marketplace</td>\n",
       "      <td>Crowd Sourcing, Freelance</td>\n",
       "      <td>Pune</td>\n",
       "      <td>Incubate Fund India and Beyond Next Ventures</td>\n",
       "      <td>pre-Series A</td>\n",
       "      <td>974,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15/07/2020</td>\n",
       "      <td>PumPumPum</td>\n",
       "      <td>Automotive Rental</td>\n",
       "      <td>Used Car-leasing platform</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Early Adapters Syndicate</td>\n",
       "      <td>Seed</td>\n",
       "      <td>292,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14/07/2020</td>\n",
       "      <td>FLYX</td>\n",
       "      <td>OTT Player</td>\n",
       "      <td>Streaming Social Network</td>\n",
       "      <td>New York and Delhi</td>\n",
       "      <td>Raj Mishra, founder of AIT Global Inc</td>\n",
       "      <td>pre-Seed</td>\n",
       "      <td>200,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13/07/2020</td>\n",
       "      <td>Open Appliances Pvt. Ltd.</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Internet-of-Things Security Solutions</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Unicorn India Ventures</td>\n",
       "      <td>Venture-Series Unknown</td>\n",
       "      <td>500,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15/08/2020</td>\n",
       "      <td>Practo</td>\n",
       "      <td>HealthTech</td>\n",
       "      <td>Health care and Wellness</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>A1A Company</td>\n",
       "      <td>Series F</td>\n",
       "      <td>32,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13/08/2020</td>\n",
       "      <td>Medlife</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Online Pharmacy</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Prasid Uno Family Trust and SC Credit Fund</td>\n",
       "      <td></td>\n",
       "      <td>23,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13/08/2020</td>\n",
       "      <td>HungerBox</td>\n",
       "      <td>FoodTech</td>\n",
       "      <td>Online Food Delivery Service</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>One97, Sabre Partners Trust, Pratithi Investme...</td>\n",
       "      <td>Series D1</td>\n",
       "      <td>1,560,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>04/08/2020</td>\n",
       "      <td>Dunzo</td>\n",
       "      <td>Hyper-local Logistics</td>\n",
       "      <td>Online Delivery Services</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Existing Backers</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>30,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11/08/2020</td>\n",
       "      <td>Terra.do</td>\n",
       "      <td>EduTech</td>\n",
       "      <td>Online Climate School, E-learning</td>\n",
       "      <td>Stanford, California,</td>\n",
       "      <td>Stanford Angels and Entrepreneurs (India), BEE...</td>\n",
       "      <td>Seed</td>\n",
       "      <td>1,400,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12/08/2020</td>\n",
       "      <td>Classplus</td>\n",
       "      <td>EduTech</td>\n",
       "      <td>E-learning, Online Tutoring</td>\n",
       "      <td>Noida</td>\n",
       "      <td>Falcon Edge</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>upto 15,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14/08/2020</td>\n",
       "      <td>Niyo</td>\n",
       "      <td>FinTech</td>\n",
       "      <td>Financial Services</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Niyo Solutions Inc.</td>\n",
       "      <td></td>\n",
       "      <td>6,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10/08/2020</td>\n",
       "      <td>ZestMoney</td>\n",
       "      <td>FinTech</td>\n",
       "      <td>Financial Services</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Primrose Hills Ventures</td>\n",
       "      <td></td>\n",
       "      <td>10,670,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>07/08/2020</td>\n",
       "      <td>FreshToHome</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Food Delivery</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Ascent Capital</td>\n",
       "      <td>Venture</td>\n",
       "      <td>16,200,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13/08/2020</td>\n",
       "      <td>Eduvanz</td>\n",
       "      <td>FinTech</td>\n",
       "      <td>Financial Services</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Sequoia India, Unitus</td>\n",
       "      <td>Series A</td>\n",
       "      <td>5,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>03/08/2020</td>\n",
       "      <td>CrowdPouch</td>\n",
       "      <td>FinTech</td>\n",
       "      <td>Financial Services</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Elina Investments Pvt. Ltd</td>\n",
       "      <td>Angel</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>04/08/2020</td>\n",
       "      <td>DrinkPrime</td>\n",
       "      <td>Water Purification</td>\n",
       "      <td>Water Purification</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Sequoia Surge, ON Mauritius</td>\n",
       "      <td>Pre-Series A</td>\n",
       "      <td>2,880,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>08/09/2020</td>\n",
       "      <td>Byju’s</td>\n",
       "      <td>EduTech</td>\n",
       "      <td>Online Tutoring</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Silver Lake, Tiger Global, General Atlantic an...</td>\n",
       "      <td>Private Equity</td>\n",
       "      <td>500,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>12/09/2020</td>\n",
       "      <td>mCaffeine</td>\n",
       "      <td>Personal Care</td>\n",
       "      <td>Skincare &amp; Haircare</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Amicus Capital Private Equity I LLP, Amicus Ca...</td>\n",
       "      <td>Series B</td>\n",
       "      <td>3,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>09/09/2020</td>\n",
       "      <td>Qshala</td>\n",
       "      <td>EduTech</td>\n",
       "      <td>Online Curiosity Platform for Kids</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Rainmatter Capital</td>\n",
       "      <td>Angel</td>\n",
       "      <td>370,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>02/09/2020</td>\n",
       "      <td>Winzo</td>\n",
       "      <td>Online Gaming</td>\n",
       "      <td>Online Gaming</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>Kalaari Capital Partners, IndigoEdge Managemen...</td>\n",
       "      <td>Series B</td>\n",
       "      <td>15,500,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>09/09/2020</td>\n",
       "      <td>Hippo Video</td>\n",
       "      <td>Video Customer Experience(CX) Platform</td>\n",
       "      <td>Video Customer Experience(CX) Platform</td>\n",
       "      <td>Newark, Delaware, United States of Amercia</td>\n",
       "      <td>Alpha Wave Incubation, Exfinity Venture Partne...</td>\n",
       "      <td>Series A</td>\n",
       "      <td>4,500,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>07/09/2020</td>\n",
       "      <td>Melorra</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Online Jewelry Store</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Shadow Holdings, Lightbox.</td>\n",
       "      <td>Debt Financing</td>\n",
       "      <td>upto 8,900,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>07/09/2020</td>\n",
       "      <td>1mg</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Online Pharmacy</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Gaja Capital, Tata Capital, Partners Group</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>100,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31/08/2020</td>\n",
       "      <td>mfine</td>\n",
       "      <td>HealthTech</td>\n",
       "      <td>On-Demand Healthcare Services</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Caretech Pte Inc</td>\n",
       "      <td>Series B</td>\n",
       "      <td>5,400,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31/08/2020</td>\n",
       "      <td>Apna</td>\n",
       "      <td>Human Resources</td>\n",
       "      <td>Recruitment Platform</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Lightspeed India and Sequoia Capital India</td>\n",
       "      <td>Series A</td>\n",
       "      <td>8,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>03/09/2020</td>\n",
       "      <td>Railofy</td>\n",
       "      <td>Transportation</td>\n",
       "      <td>WL &amp; RAC protection platform</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Chiratae Ventures</td>\n",
       "      <td>Seed</td>\n",
       "      <td>950,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>08/09/2020</td>\n",
       "      <td>Cell Propulsion</td>\n",
       "      <td>Automobile</td>\n",
       "      <td>Electric Mobility Solutions</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>growX Ventures and Micelio</td>\n",
       "      <td>pre-Series A</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date               Startup Name  \\\n",
       "0   15/07/2020                   Flipkart   \n",
       "1   16/07/2020                    Vedantu   \n",
       "2   16/07/2020                       Crio   \n",
       "3   14/07/2020                    goDutch   \n",
       "4   13/07/2020                   Mystifly   \n",
       "5   09/07/2020               JetSynthesys   \n",
       "6   10/07/2020                   gigIndia   \n",
       "7   15/07/2020                  PumPumPum   \n",
       "8   14/07/2020                       FLYX   \n",
       "9   13/07/2020  Open Appliances Pvt. Ltd.   \n",
       "10  15/08/2020                     Practo   \n",
       "11  13/08/2020                    Medlife   \n",
       "12  13/08/2020                  HungerBox   \n",
       "13  04/08/2020                      Dunzo   \n",
       "14  11/08/2020                   Terra.do   \n",
       "15  12/08/2020                  Classplus   \n",
       "16  14/08/2020                       Niyo   \n",
       "17  10/08/2020                  ZestMoney   \n",
       "18  07/08/2020                FreshToHome   \n",
       "19  13/08/2020                    Eduvanz   \n",
       "20  03/08/2020                 CrowdPouch   \n",
       "21  04/08/2020                 DrinkPrime   \n",
       "22  08/09/2020                     Byju’s   \n",
       "23  12/09/2020                  mCaffeine   \n",
       "24  09/09/2020                     Qshala   \n",
       "25  02/09/2020                      Winzo   \n",
       "26  09/09/2020                Hippo Video   \n",
       "27  07/09/2020                    Melorra   \n",
       "28  07/09/2020                        1mg   \n",
       "29  31/08/2020                      mfine   \n",
       "30  31/08/2020                       Apna   \n",
       "31  03/09/2020                    Railofy   \n",
       "32  08/09/2020            Cell Propulsion   \n",
       "\n",
       "                         Industry/Vertical  \\\n",
       "0                               E-commerce   \n",
       "1                                  EduTech   \n",
       "2                                  EduTech   \n",
       "3                                  FinTech   \n",
       "4                      Airfare Marketplace   \n",
       "5                 Gaming and Entertainment   \n",
       "6                              Marketplace   \n",
       "7                        Automotive Rental   \n",
       "8                               OTT Player   \n",
       "9                   Information Technology   \n",
       "10                              HealthTech   \n",
       "11                              E-commerce   \n",
       "12                                FoodTech   \n",
       "13                   Hyper-local Logistics   \n",
       "14                                 EduTech   \n",
       "15                                 EduTech   \n",
       "16                                 FinTech   \n",
       "17                                 FinTech   \n",
       "18                              E-commerce   \n",
       "19                                 FinTech   \n",
       "20                                 FinTech   \n",
       "21                      Water Purification   \n",
       "22                                 EduTech   \n",
       "23                           Personal Care   \n",
       "24                                 EduTech   \n",
       "25                           Online Gaming   \n",
       "26  Video Customer Experience(CX) Platform   \n",
       "27                              E-commerce   \n",
       "28                              E-commerce   \n",
       "29                              HealthTech   \n",
       "30                         Human Resources   \n",
       "31                          Transportation   \n",
       "32                              Automobile   \n",
       "\n",
       "                                         Sub-Vertical  \\\n",
       "0                                          E-commerce   \n",
       "1                                     Online Tutoring   \n",
       "2                    Learning Platform for Developers   \n",
       "3                                      Group Payments   \n",
       "4   Ticketing, Airline Retailing, and Post-Ticketi...   \n",
       "5                            Gaming and Entertainment   \n",
       "6                           Crowd Sourcing, Freelance   \n",
       "7                           Used Car-leasing platform   \n",
       "8                            Streaming Social Network   \n",
       "9               Internet-of-Things Security Solutions   \n",
       "10                           Health care and Wellness   \n",
       "11                                    Online Pharmacy   \n",
       "12                       Online Food Delivery Service   \n",
       "13                           Online Delivery Services   \n",
       "14                  Online Climate School, E-learning   \n",
       "15                        E-learning, Online Tutoring   \n",
       "16                                 Financial Services   \n",
       "17                                 Financial Services   \n",
       "18                                      Food Delivery   \n",
       "19                                 Financial Services   \n",
       "20                                 Financial Services   \n",
       "21                                 Water Purification   \n",
       "22                                    Online Tutoring   \n",
       "23                                Skincare & Haircare   \n",
       "24                 Online Curiosity Platform for Kids   \n",
       "25                                      Online Gaming   \n",
       "26             Video Customer Experience(CX) Platform   \n",
       "27                               Online Jewelry Store   \n",
       "28                                    Online Pharmacy   \n",
       "29                      On-Demand Healthcare Services   \n",
       "30                               Recruitment Platform   \n",
       "31                       WL & RAC protection platform   \n",
       "32                        Electric Mobility Solutions   \n",
       "\n",
       "                                      Location  \\\n",
       "0                                    Bangalore   \n",
       "1                                    Bangalore   \n",
       "2                                    Bangalore   \n",
       "3                                       Mumbai   \n",
       "4                      Singapore and Bangalore   \n",
       "5                                         Pune   \n",
       "6                                         Pune   \n",
       "7                                      Gurgaon   \n",
       "8                           New York and Delhi   \n",
       "9                                    Bangalore   \n",
       "10                                   Bangalore   \n",
       "11                                   Bangalore   \n",
       "12                                   Bangalore   \n",
       "13                                   Bangalore   \n",
       "14                       Stanford, California,   \n",
       "15                                       Noida   \n",
       "16                                   Bangalore   \n",
       "17                                   Bangalore   \n",
       "18                                   Bangalore   \n",
       "19                                      Mumbai   \n",
       "20                                   Bangalore   \n",
       "21                                   Bangalore   \n",
       "22                                   Bangalore   \n",
       "23                                      Mumbai   \n",
       "24                                   Bangalore   \n",
       "25                                   New Delhi   \n",
       "26  Newark, Delaware, United States of Amercia   \n",
       "27                                   Bangalore   \n",
       "28                                     Gurgaon   \n",
       "29                                   Bangalore   \n",
       "30                                   Bangalore   \n",
       "31                                      Mumbai   \n",
       "32                                   Bangalore   \n",
       "\n",
       "                                             Investor         Investment Type  \\\n",
       "0                                         Walmart Inc                     M&A   \n",
       "1                                   Coatue Management                Series D   \n",
       "2                                         021 Capital            pre-Series A   \n",
       "3   Matrix India,Y Combinator, Global Founders Cap...                    Seed   \n",
       "4                                    Recruit Co. Ltd.            pre-Series B   \n",
       "5            Adar Poonawalla and Kris Gopalakrishnan.  Venture-Series Unknown   \n",
       "6        Incubate Fund India and Beyond Next Ventures            pre-Series A   \n",
       "7                            Early Adapters Syndicate                    Seed   \n",
       "8               Raj Mishra, founder of AIT Global Inc                pre-Seed   \n",
       "9                              Unicorn India Ventures  Venture-Series Unknown   \n",
       "10                                        A1A Company                Series F   \n",
       "11         Prasid Uno Family Trust and SC Credit Fund                           \n",
       "12  One97, Sabre Partners Trust, Pratithi Investme...               Series D1   \n",
       "13                                   Existing Backers             In Progress   \n",
       "14  Stanford Angels and Entrepreneurs (India), BEE...                    Seed   \n",
       "15                                        Falcon Edge             In Progress   \n",
       "16                                Niyo Solutions Inc.                           \n",
       "17                            Primrose Hills Ventures                           \n",
       "18                                     Ascent Capital                 Venture   \n",
       "19                              Sequoia India, Unitus                Series A   \n",
       "20                         Elina Investments Pvt. Ltd                   Angel   \n",
       "21                        Sequoia Surge, ON Mauritius            Pre-Series A   \n",
       "22  Silver Lake, Tiger Global, General Atlantic an...          Private Equity   \n",
       "23  Amicus Capital Private Equity I LLP, Amicus Ca...                Series B   \n",
       "24                                 Rainmatter Capital                   Angel   \n",
       "25  Kalaari Capital Partners, IndigoEdge Managemen...                Series B   \n",
       "26  Alpha Wave Incubation, Exfinity Venture Partne...                Series A   \n",
       "27                         Shadow Holdings, Lightbox.          Debt Financing   \n",
       "28         Gaja Capital, Tata Capital, Partners Group             In Progress   \n",
       "29                                   Caretech Pte Inc                Series B   \n",
       "30         Lightspeed India and Sequoia Capital India                Series A   \n",
       "31                                  Chiratae Ventures                    Seed   \n",
       "32                         growX Ventures and Micelio            pre-Series A   \n",
       "\n",
       "     Amount(in USD)  \n",
       "0     1,200,000,000  \n",
       "1       100,000,000  \n",
       "2           934,160  \n",
       "3         1,700,000  \n",
       "4         3,300,000  \n",
       "5           400,000  \n",
       "6           974,200  \n",
       "7           292,800  \n",
       "8           200,000  \n",
       "9           500,000  \n",
       "10       32,000,000  \n",
       "11       23,000,000  \n",
       "12        1,560,000  \n",
       "13       30,000,000  \n",
       "14        1,400,000  \n",
       "15  upto 15,000,000  \n",
       "16        6,000,000  \n",
       "17       10,670,000  \n",
       "18       16,200,000  \n",
       "19        5,000,000  \n",
       "20               NA  \n",
       "21        2,880,000  \n",
       "22      500,000,000  \n",
       "23        3,000,000  \n",
       "24          370,000  \n",
       "25       15,500,000  \n",
       "26        4,500,000  \n",
       "27   upto 8,900,000  \n",
       "28      100,000,000  \n",
       "29        5,400,000  \n",
       "30        8,000,000  \n",
       "31          950,000  \n",
       "32               NA  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fund_dict = {}\n",
    "fund_dict['Date'] = []\n",
    "fund_dict['Startup Name'] = []\n",
    "fund_dict['Industry/Vertical'] = []\n",
    "fund_dict['Sub-Vertical'] = []\n",
    "fund_dict['Location'] = []\n",
    "fund_dict['Investor'] = []\n",
    "fund_dict['Investment Type'] = []\n",
    "fund_dict['Amount(in USD)'] = []\n",
    "\n",
    "for i in range(48,51):\n",
    "    driver.find_element_by_xpath('//div[@id=\"tablepress-{}_wrapper\"]/div/label/select/option[4]'.format(i)).click()\n",
    "\n",
    "    # Date\n",
    "    dt = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[2]'.format(i))\n",
    "    for d in dt:\n",
    "        fund_dict['Date'].append(d.text)\n",
    "\n",
    "    # Startup Name\n",
    "    sn = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[3]'.format(i))\n",
    "    for n in sn:\n",
    "        fund_dict['Startup Name'].append(n.text)\n",
    "    \n",
    "    # Industry/Vertical\n",
    "    ind = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[4]'.format(i))\n",
    "    for n in ind:\n",
    "        fund_dict['Industry/Vertical'].append(n.text)\n",
    "    \n",
    "    # Sub-Vertical\n",
    "    sv = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[5]'.format(i))\n",
    "    for s in sv:\n",
    "        fund_dict['Sub-Vertical'].append(s.text)\n",
    "\n",
    "    # Location\n",
    "    loc = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[6]'.format(i))\n",
    "    for l in loc:\n",
    "        fund_dict['Location'].append(l.text)\n",
    "    \n",
    "    # Investor\n",
    "    inv = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[7]'.format(i))\n",
    "    for n in inv:\n",
    "        fund_dict['Investor'].append(n.text)\n",
    "    \n",
    "    # Investment Type\n",
    "    invt = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[8]'.format(i))\n",
    "    for n in invt:\n",
    "        fund_dict['Investment Type'].append(n.text)\n",
    "    \n",
    "    # Amount\n",
    "    amt = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[9]'.format(i))\n",
    "    for a in amt:\n",
    "        fund_dict['Amount(in USD)'].append(a.text)\n",
    "    \n",
    "fund_df = pd.DataFrame(fund_dict)\n",
    "fund_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f286820",
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_df.to_csv(\"trak_in.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd27421",
   "metadata": {},
   "source": [
    "# Q7 : Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9659af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Administrator\\Downloads\\chromedriver.exe\")\n",
    "# opening the url digit.in\n",
    "driver.get('https://www.digit.in/')\n",
    "time.sleep(2)\n",
    "# searching for best Laptop\n",
    "best_gam_laptops = driver.find_element_by_xpath(\"//div[@class='listing_container']//ul//li[9]\").click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72203b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list\n",
    "Laptop_Name = []\n",
    "Operating_sys = []\n",
    "Display = []\n",
    "Processor = []\n",
    "Memory = []\n",
    "Weight = []\n",
    "Dimensions = []\n",
    "Graph_proc = []\n",
    "Price = []\n",
    "\n",
    "#scraping the data of laptop names\n",
    "laptop_name = driver.find_elements_by_xpath(\"//div[@class='right-container']/div/a/h3\")\n",
    "for name in laptop_name:\n",
    "    Laptop_Name.append(name.text)\n",
    "    \n",
    "#scraping the data of operating system\n",
    "try:\n",
    "    op_sys = driver.find_elements_by_xpath(\"//div[@class='product-detail']/div/ul/li[1]/div/div\")\n",
    "    for os in op_sys:\n",
    "        Operating_sys.append(os.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "#scraping data of display of the Laptop\n",
    "try:\n",
    "    display = driver.find_elements_by_xpath(\"//div[@class='product-detail']/div/ul/li[2]/div/div\")\n",
    "    for disp in display:\n",
    "        Display.append(disp.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "# scraping data of processor\n",
    "try:\n",
    "    processor = driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[5]/td[3]\")\n",
    "    for pro in processor:\n",
    "        Processor.append(pro.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "# scraping the data of memory\n",
    "try:\n",
    "    memory = driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[6]/td[3]\")\n",
    "    for memo in memory:\n",
    "        Memory.append(memo.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "# scraping data of weight\n",
    "try:\n",
    "    weight = driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[7]/td[3]\")\n",
    "    for wgt in weight:\n",
    "        Weight.append(wgt.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "# scraping data of dimensions\n",
    "try:\n",
    "    dimension = driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[8]/td[3]\")\n",
    "    for dim in dimension:\n",
    "        Dimensions.append(dim.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "# scraping data of graph processor\n",
    "try:\n",
    "    graph = driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[9]/td[3]\")\n",
    "    for gra in graph:\n",
    "        Graph_proc.append(gra.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "# scraping the data of price\n",
    "try:\n",
    "    price = driver.find_elements_by_xpath(\"//td[@class='smprice']\")\n",
    "    for pri in price:\n",
    "        Price.append(pri.text.replace('₹ ','Rs'))\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "print(len(Laptop_Name),len(Operating_sys),len(Display),len(Processor),len(Memory),len(Weight),len(Dimensions),len(Graph_proc),\n",
    "len(Price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d17dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating DataFrame for scraped data\n",
    "Gaming_Laptop=pd.DataFrame({})\n",
    "Gaming_Laptop['Laptop Name'] = Laptop_Name\n",
    "Gaming_Laptop['Operating System'] =Operating_sys\n",
    "Gaming_Laptop['Display'] = Display\n",
    "Gaming_Laptop['Processor'] = Processor\n",
    "Gaming_Laptop['Memory'] = Memory\n",
    "Gaming_Laptop['Weight'] = Weight\n",
    "Gaming_Laptop['Dimensions'] = Dimensions\n",
    "Gaming_Laptop['Graphical Processor'] = Graph_proc\n",
    "Gaming_Laptop['Price'] = Price\n",
    "Gaming_Laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d9496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the data to csv\n",
    "Gaming_Laptop.to_csv(\"Gaming_Laptops.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc57d6d",
   "metadata": {},
   "source": [
    "# Q8 : Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70c7c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Administrator\\Downloads\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "361934eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the specified url\n",
    "url = \"https://www.forbes.com/?sh=41bd46d2254c\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "580ee8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's get option button from the page\n",
    "opt_btn = driver.find_element_by_xpath(\"//div[@class='header__left']//button\")\n",
    "opt_btn.click()\n",
    "time.sleep(3)\n",
    "\n",
    "#select billionaires from options\n",
    "blns = driver.find_element_by_xpath(\"/html/body/div[1]/header/nav/div[3]/ul/li[1]\")\n",
    "blns.click()\n",
    "time.sleep(3)\n",
    "#select world billionaire\n",
    "bln_list = driver.find_element_by_xpath(\"/html/body/div[1]/header/nav/div[3]/ul/li[1]/div[2]/ul/li[2]/a\")\n",
    "bln_list.click()\n",
    "time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc3013e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping required data from the web page\n",
    "# creating empty lists\n",
    "Rank = []\n",
    "Person_Name = []\n",
    "Net_worth = []\n",
    "Age = []\n",
    "Citizenship = []\n",
    "Source = []\n",
    "Industry = []\n",
    "\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    # scraping the data of rank of the billionaires\n",
    "    rank_tag = driver.find_elements_by_xpath(\"//div[@class='rank']\")\n",
    "    for rank in rank_tag:\n",
    "        Rank.append(rank.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    " \n",
    "    # scraping the data  of names of the billionaires\n",
    "    name_tag = driver.find_elements_by_xpath(\"//div[@class='personName']/div\")\n",
    "    for name in name_tag:\n",
    "        Person_Name.append(name.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # scraping the data of age of the billionaires\n",
    "    age_tag = driver.find_elements_by_xpath(\"//div[@class='age']/div\")\n",
    "    for age in age_tag:\n",
    "        Age.append(age.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # scraping the data of citizenship of the billionaires\n",
    "    cit_tag = driver.find_elements_by_xpath(\"//div[@class='countryOfCitizenship']\")\n",
    "    for cit in cit_tag:\n",
    "        Citizenship.append(cit.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # scraping the data of source of income of the billionaires\n",
    "    sour_tag = driver.find_elements_by_xpath(\"//div[@class='source']\")\n",
    "    for sour in sour_tag:\n",
    "        Source.append(sour.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # scraping data of industry of the billionaires\n",
    "    ind_tag = driver.find_elements_by_xpath(\"//div[@class='category']//div\")\n",
    "    for ind in ind_tag:\n",
    "        Industry.append(ind.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # scraping data of net_worth of billionaires\n",
    "    net_tag = driver.find_elements_by_xpath(\"//div[@class='netWorth']/div\")\n",
    "    for net in net_tag:\n",
    "        Net_worth.append(net.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # clicking on next button\n",
    "    try:\n",
    "        next_button = driver.find_element_by_xpath(\"//button[@class='pagination-btn pagination-btn--next ']\")\n",
    "        next_button.click()\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c1aa53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2755 2755 2755 2755 2755 2755 2755\n"
     ]
    }
   ],
   "source": [
    "print(len(Rank),\n",
    "len(Person_Name),\n",
    "len(Net_worth),\n",
    "len(Age),\n",
    "len(Citizenship),\n",
    "len(Source),\n",
    "len(Industry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "034e966d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Net Worth</th>\n",
       "      <th>Age</th>\n",
       "      <th>Citizenship</th>\n",
       "      <th>Source</th>\n",
       "      <th>Industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>Jeff Bezos</td>\n",
       "      <td>$177 B</td>\n",
       "      <td>57</td>\n",
       "      <td>United States</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>Elon Musk</td>\n",
       "      <td>$151 B</td>\n",
       "      <td>49</td>\n",
       "      <td>United States</td>\n",
       "      <td>Tesla, SpaceX</td>\n",
       "      <td>Automotive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>Bernard Arnault &amp; family</td>\n",
       "      <td>$150 B</td>\n",
       "      <td>72</td>\n",
       "      <td>France</td>\n",
       "      <td>LVMH</td>\n",
       "      <td>Fashion &amp; Retail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>$124 B</td>\n",
       "      <td>65</td>\n",
       "      <td>United States</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>Mark Zuckerberg</td>\n",
       "      <td>$97 B</td>\n",
       "      <td>36</td>\n",
       "      <td>United States</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750</th>\n",
       "      <td>2674.</td>\n",
       "      <td>Daniel Yong Zhang</td>\n",
       "      <td>$1 B</td>\n",
       "      <td>49</td>\n",
       "      <td>China</td>\n",
       "      <td>e-commerce</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2751</th>\n",
       "      <td>2674.</td>\n",
       "      <td>Zhang Yuqiang</td>\n",
       "      <td>$1 B</td>\n",
       "      <td>65</td>\n",
       "      <td>China</td>\n",
       "      <td>Fiberglass</td>\n",
       "      <td>Manufacturing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2752</th>\n",
       "      <td>2674.</td>\n",
       "      <td>Zhao Meiguang</td>\n",
       "      <td>$1 B</td>\n",
       "      <td>58</td>\n",
       "      <td>China</td>\n",
       "      <td>gold mining</td>\n",
       "      <td>Metals &amp; Mining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2753</th>\n",
       "      <td>2674.</td>\n",
       "      <td>Zhong Naixiong</td>\n",
       "      <td>$1 B</td>\n",
       "      <td>58</td>\n",
       "      <td>China</td>\n",
       "      <td>conglomerate</td>\n",
       "      <td>Diversified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2754</th>\n",
       "      <td>2674.</td>\n",
       "      <td>Zhou Wei family</td>\n",
       "      <td>$1 B</td>\n",
       "      <td>54</td>\n",
       "      <td>China</td>\n",
       "      <td>Software</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2755 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Rank                      Name Net Worth Age    Citizenship  \\\n",
       "0        1.                Jeff Bezos    $177 B  57  United States   \n",
       "1        2.                 Elon Musk    $151 B  49  United States   \n",
       "2        3.  Bernard Arnault & family    $150 B  72         France   \n",
       "3        4.                Bill Gates    $124 B  65  United States   \n",
       "4        5.           Mark Zuckerberg     $97 B  36  United States   \n",
       "...     ...                       ...       ...  ..            ...   \n",
       "2750  2674.         Daniel Yong Zhang      $1 B  49          China   \n",
       "2751  2674.             Zhang Yuqiang      $1 B  65          China   \n",
       "2752  2674.             Zhao Meiguang      $1 B  58          China   \n",
       "2753  2674.            Zhong Naixiong      $1 B  58          China   \n",
       "2754  2674.           Zhou Wei family      $1 B  54          China   \n",
       "\n",
       "             Source          Industry  \n",
       "0            Amazon        Technology  \n",
       "1     Tesla, SpaceX        Automotive  \n",
       "2              LVMH  Fashion & Retail  \n",
       "3         Microsoft        Technology  \n",
       "4          Facebook        Technology  \n",
       "...             ...               ...  \n",
       "2750     e-commerce        Technology  \n",
       "2751     Fiberglass     Manufacturing  \n",
       "2752    gold mining   Metals & Mining  \n",
       "2753   conglomerate       Diversified  \n",
       "2754       Software        Technology  \n",
       "\n",
       "[2755 rows x 7 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# framing Data\n",
    "Billionaires = pd.DataFrame({})\n",
    "Billionaires['Rank'] = Rank\n",
    "Billionaires['Name'] = Person_Name\n",
    "Billionaires['Net Worth'] = Net_worth\n",
    "Billionaires['Age'] = Age\n",
    "Billionaires['Citizenship'] = Citizenship\n",
    "Billionaires['Source'] = Source\n",
    "Billionaires['Industry'] = Industry\n",
    "Billionaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "771a80b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving dataset in csv\n",
    "Billionaires.to_csv('Forbes_Billionaires.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa09c238",
   "metadata": {},
   "source": [
    "# Q9 : Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6512d674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Administrator\\Downloads\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6cd68235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the youtube.com\n",
    "url = \"https://www.youtube.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "# finding element for search bar\n",
    "search_bar = driver.find_element_by_xpath(\"//div[@class='ytd-searchbox-spt']/input\")\n",
    "search_bar.send_keys(\"GOT\")      # entering video name\n",
    "time.sleep(2)\n",
    "#clicking on search button\n",
    "search_btn = driver.find_element_by_id(\"search-icon-legacy\")\n",
    "search_btn.click()\n",
    "time.sleep(2)\n",
    "# clicking on first video\n",
    "video = driver.find_element_by_xpath(\"//yt-formatted-string[@class='style-scope ytd-video-renderer']\")\n",
    "video.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81e3c52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260 280 280\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Comment Time</th>\n",
       "      <th>Comment Upvotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I just noticed that Sansa wasn't there when he...</td>\n",
       "      <td>5 years ago</td>\n",
       "      <td>1.7K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I need a 10 hour version of Ramsey getting bea...</td>\n",
       "      <td>5 years ago</td>\n",
       "      <td>1.8K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jon Snow = leads from the front. \\nRamsay Bolt...</td>\n",
       "      <td>5 years ago</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There is such a thing as Jon talking to Sansa ...</td>\n",
       "      <td>5 years ago</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lyanna Mormont's death stare was the best.</td>\n",
       "      <td>5 years ago</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>Kalami na lang duslakon. Saw this scene multip...</td>\n",
       "      <td>2 years ago</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0:52 Man they really missed a perfect pun by n...</td>\n",
       "      <td>5 years ago</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Ramsey is a freakin good actor. Makes you hate...</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>where was brienne and pod this episode? they s...</td>\n",
       "      <td>5 years ago</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>best episode so far.</td>\n",
       "      <td>5 years ago</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Comment Comment Time  \\\n",
       "0    I just noticed that Sansa wasn't there when he...  5 years ago   \n",
       "1    I need a 10 hour version of Ramsey getting bea...  5 years ago   \n",
       "2    Jon Snow = leads from the front. \\nRamsay Bolt...  5 years ago   \n",
       "3    There is such a thing as Jon talking to Sansa ...  5 years ago   \n",
       "4           Lyanna Mormont's death stare was the best.  5 years ago   \n",
       "..                                                 ...          ...   \n",
       "235  Kalami na lang duslakon. Saw this scene multip...  2 years ago   \n",
       "236  0:52 Man they really missed a perfect pun by n...  5 years ago   \n",
       "237  Ramsey is a freakin good actor. Makes you hate...  1 month ago   \n",
       "238  where was brienne and pod this episode? they s...  5 years ago   \n",
       "239                               best episode so far.  5 years ago   \n",
       "\n",
       "    Comment Upvotes  \n",
       "0              1.7K  \n",
       "1              1.8K  \n",
       "2               657  \n",
       "3               593  \n",
       "4               264  \n",
       "..              ...  \n",
       "235                  \n",
       "236               1  \n",
       "237                  \n",
       "238               5  \n",
       "239                  \n",
       "\n",
       "[240 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1000 times we scroll down by 10000 in order to generate more comments\n",
    "for _ in range(1000):\n",
    "    driver.execute_script(\"window.scrollBy(0,10000)\")\n",
    "# creating empty lists\n",
    "comments = []\n",
    "comment_time = []\n",
    "Time = []\n",
    "Likes = []\n",
    "No_of_Likes = []\n",
    "\n",
    "# scrape comments\n",
    "cm = driver.find_elements_by_id(\"content-text\")\n",
    "for i in cm:\n",
    "    if i.text is None:\n",
    "        comments.append(\"--\")\n",
    "    else:\n",
    "        comments.append(i.text)\n",
    "time.sleep(4)\n",
    "\n",
    "\n",
    "# scrape time when comment was posted\n",
    "tm = driver.find_elements_by_xpath(\"//a[contains(text(),'ago')]\")\n",
    "for i in tm:\n",
    "    Time.append(i.text)\n",
    "    \n",
    "for i in range(0,len(Time),2):\n",
    "    comment_time.append(Time[i])\n",
    "time.sleep(4)\n",
    "# scrape the comment likes\n",
    "like = driver.find_elements_by_xpath(\"//span[@class='style-scope ytd-comment-action-buttons-renderer']\")\n",
    "for i in like:\n",
    "    Likes.append(i.text)\n",
    "    \n",
    "for i in range(1,len(Likes),2):\n",
    "    No_of_Likes.append(Likes[i])\n",
    "print(len(comments),len(comment_time),len(No_of_Likes))\n",
    "\n",
    "# creating dataframe for scraped data\n",
    "Youtube = pd.DataFrame({})\n",
    "Youtube['Comment'] = comments[:240]\n",
    "Youtube['Comment Time'] = comment_time[:240]\n",
    "Youtube['Comment Upvotes'] = No_of_Likes[:240]\n",
    "Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "232fdd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the dataframe to csv\n",
    "Youtube.to_csv(\"YoutubeComments.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a133278",
   "metadata": {},
   "source": [
    "# Q10 : Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "437cde2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Administrator\\Downloads\\chromedriver.exe\")\n",
    "driver.get(\"https://www.hostelworld.com/s?q=London,%20England&country=England&city=London&type=city&id=3&from=2021-07-23&to=2021-07-26&guests=2&page=1\")\n",
    "driver.maximize_window()\n",
    "name = driver.find_elements_by_xpath(\"//h2[@class='title title-6']/a\")\n",
    "hotel_name = []\n",
    "for i in name:\n",
    "    hotel_name.append(i.text)\n",
    "\n",
    "        \n",
    "distance = driver.find_elements_by_class_name('description')\n",
    "distance_from_city = []\n",
    "for i in distance:\n",
    "    distance_from_city.append(i.text)\n",
    "    \n",
    "rat = driver.find_elements_by_xpath(\"//div[@class='score orange big']\")\n",
    "ratings = []\n",
    "for i  in rat:\n",
    "    try:\n",
    "        ratings.append(i.text)\n",
    "    except NoSuchElementException as E:\n",
    "        ratings.append(\"__\")\n",
    "total_rates = driver.find_elements_by_xpath(\"//div[@class='reviews']\")\n",
    "total_rating =[]\n",
    "for i in total_rates:\n",
    "    total_rating.append(i.text)\n",
    "    \n",
    "over_all = driver.find_elements_by_xpath(\"//div[@class='keyword']\")\n",
    "over_all_review =[]\n",
    "for i in over_all:\n",
    "    over_all_review.append(i.text)\n",
    "    \n",
    "pp =driver.find_elements_by_xpath(\"//div[@class='price title-5']\")\n",
    "\n",
    "privates_price =[]\n",
    "for i in pp:\n",
    "    privates_price.append(i.text)\n",
    "    \n",
    "privates_price = privates_price[0::2]\n",
    "dp =driver.find_elements_by_xpath(\"//div[@class='price title-5']\")\n",
    "\n",
    "dorms_price =[]\n",
    "for i in dp:\n",
    "    dorms_price.append(i.text)\n",
    "    \n",
    "dorms_price = dorms_price[1::2]\n",
    "desc =driver.find_elements_by_xpath(\"//div[@class='rating-factors prop-card-tablet rating-factors small']\")\n",
    "\n",
    "description =[]\n",
    "for i in desc:\n",
    "    description.append(i.text)\n",
    "    \n",
    "fasc =driver.find_elements_by_xpath(\"//div[@class='facilities-label facilities']\")\n",
    "\n",
    "facilities =[]\n",
    "for i in fasc:\n",
    "    facilities.append(i.text)\n",
    "    \n",
    "import pandas as pd\n",
    "hotel_details=pd.DataFrame({})\n",
    "hotel_details['Hostel_Name']= hotel_name[0:20]\n",
    "hotel_details['Type and Distance']= distance_from_city[0:20]\n",
    "hotel_details['Ratings']= ratings[0:20]\n",
    "hotel_details['Total_reviews']= total_rating[0:20]\n",
    "hotel_details['Overall_reviews']= over_all_review[0:20]\n",
    "hotel_details['Private Prices']= privates_price[0:20]\n",
    "hotel_details['Dorms Prices']= dorms_price[0:20]\n",
    "hotel_details['Description']= description[0:20]\n",
    "hotel_details['Facilities']= facilities[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "23a80aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('Facilities')\n",
    "len('Type and Distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d624430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "hotel_details=pd.DataFrame({})\n",
    "hotel_details['Hostel_Name']= hotel_name[0:7]\n",
    "hotel_details['Type and Distance']= distance_from_city[0:7]\n",
    "hotel_details['Ratings']= ratings[0:7]\n",
    "hotel_details['Total_reviews']= total_rating[0:7]\n",
    "hotel_details['Overall_reviews']= over_all_review[0:7]\n",
    "hotel_details['Private Prices']= privates_price[0:7]\n",
    "hotel_details['Dorms Prices']= dorms_price[0:7]\n",
    "hotel_details['Description']= description[0:7]\n",
    "hotel_details['Facilities']= facilities[0:7]\n",
    "hotel_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c1c8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
